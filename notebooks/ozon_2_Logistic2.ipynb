{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eb31b7c-60d9-4ab7-b721-4c7e37b79db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ozon_4_.ipynb\n",
      "../data/sample_submit.csv\n",
      "../data/submission_lightBGM.csv\n",
      "../data/submission_lightBGM2.csv\n",
      "../data/submission_lightBGM2_2.csv\n",
      "../data/submission_lightBGM3.csv\n",
      "../data/submission_Logistic.csv\n",
      "../data/submission_Logistic2.csv\n",
      "../data/submission_Logistic3.csv\n",
      "../data/test.tsv\n",
      "../data/train.tsv\n",
      "../data/.ipynb_checkpoints\\ozon_4_-checkpoint.ipynb\n",
      "../data/.ipynb_checkpoints\\sample_submit-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic2-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\test-checkpoint.tsv\n",
      "../data/.ipynb_checkpoints\\train-checkpoint.tsv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../data/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c24b6ef6-e994-4114-827e-b92232f296ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame._add_numeric_operations.<locals>.sum of               id  WSR0  WSR1  WSR2  WSR3  WSR4  WSR5  WSR6  WSR7  WSR8  ...  \\\n",
       "Date                                                                    ...   \n",
       "1998-04-05    94   0.4   0.5   2.1   2.2   2.5   2.4   2.1   2.9   3.6  ...   \n",
       "1998-04-11   100   0.0   0.6   0.4   0.3   0.1   0.3   0.2   1.4   2.6  ...   \n",
       "1998-04-20   109   1.8   0.3   0.1   0.1   0.1   0.2   0.2   0.7   0.9  ...   \n",
       "1998-04-23   112   0.5   0.1   0.1   0.1   0.1   0.2   0.3   0.8   1.2  ...   \n",
       "1998-04-25   114   3.1   2.4   2.4   3.0   3.4   3.4   3.9   4.5   5.5  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "2001-05-23  1225   0.8   0.3   0.2   0.4   0.3   1.4   0.9   1.5   1.9  ...   \n",
       "2001-06-15  1248   2.2   1.7   0.8   3.8   4.0   4.1   2.0   1.9   1.8  ...   \n",
       "2001-06-16  1249   0.4   0.4   0.1   0.1   0.0   0.4   0.5   0.6   1.3  ...   \n",
       "2001-06-18  1251   0.4   0.7   0.5   0.6   0.8   1.0   1.6   1.5   2.3  ...   \n",
       "2001-06-19  1252   0.7   0.7   0.1   0.1   0.2   0.4   0.4   0.4   0.8  ...   \n",
       "\n",
       "              U50    V50    HT50     KI     TT      SLP  SLP_  Precp  OZONE  \\\n",
       "Date                                                                          \n",
       "1998-04-05  20.91  -3.90  5755.0 -15.90  19.40  10140.0  20.0   0.00      1   \n",
       "1998-04-11  17.27 -12.27  5795.0 -12.60  24.20  10220.0  45.0   0.00      1   \n",
       "1998-04-20  20.36   2.61  5740.0  -3.50  30.60  10180.0  35.0   0.00      1   \n",
       "1998-04-23  16.78 -17.99  5680.0  -2.40  37.60  10195.0 -10.0   0.00      1   \n",
       "1998-04-25   9.22  -5.96  5790.0   7.10  35.40  10165.0 -25.0   0.00      1   \n",
       "...           ...    ...     ...    ...    ...      ...   ...    ...    ...   \n",
       "2001-05-23  17.58  -2.33  5795.0   3.40  35.70  10150.0  25.0   0.00      1   \n",
       "2001-06-15   7.11   7.01  5865.0  34.10  48.80  10150.0  50.0   3.33      1   \n",
       "2001-06-16   6.40  -5.96  5900.0   0.25  44.35  10185.0  35.0   0.00      1   \n",
       "2001-06-18  -4.00  -4.00  5900.0  -1.70  41.40  10180.0 -15.0   0.00      1   \n",
       "2001-06-19  -2.25  -3.93  5880.0  26.65  45.75  10165.0 -15.0   0.00      1   \n",
       "\n",
       "             type  \n",
       "Date               \n",
       "1998-04-05  train  \n",
       "1998-04-11  train  \n",
       "1998-04-20  train  \n",
       "1998-04-23  train  \n",
       "1998-04-25  train  \n",
       "...           ...  \n",
       "2001-05-23  train  \n",
       "2001-06-15  train  \n",
       "2001-06-16  train  \n",
       "2001-06-18  train  \n",
       "2001-06-19  train  \n",
       "\n",
       "[111 rows x 75 columns]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_table('../data/train.tsv', index_col='Date', parse_dates=True)\n",
    "test_df = pd.read_table('../data/test.tsv', index_col='Date', parse_dates=True)\n",
    "sample_sub = pd.read_csv('../data/sample_submit.csv')\n",
    "\n",
    "# set type label\n",
    "train_df['type'] = 'train'\n",
    "test_df['type'] = 'test'\n",
    "\n",
    "# all data\n",
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# OZONEが高い日の数\n",
    "train_df[train_df[\"OZONE\"]==1.0].sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad5c3d-d96a-40f1-85d0-ea6b608575a8",
   "metadata": {},
   "source": [
    "## 学習する特徴量を作成\n",
    "#### __欠損値処理__\n",
    "→全部平均値で補完\n",
    "\n",
    "#### __特徴量の削除/追加__\n",
    "ピアソン相関から、  \n",
    "[削除]  \n",
    "- 時間ごとの気温\"T0\"~\"T23\"を消す\n",
    "- 時間ごとの風速\"WSR0\"~\"WSR24\"を消す\n",
    "- 海面気圧の前日からの変化\"SLP_\"を消す  \n",
    "\n",
    "[とりあえず追加]\n",
    "- 風速の標準偏差を追加する\n",
    "- 気温の標準偏差を追加する\n",
    "  \n",
    "計26個\n",
    "\n",
    "#### __データ変換__\n",
    "→とりあえず全部標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "308334c6-5d7e-4bf0-b7c3-b1d0c72582b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df : \n",
      "              WSR_PK    WSR_AV      T_PK      T_AV           T85  \\\n",
      "Date                                                               \n",
      "1998-01-01  1.015406  0.727247 -0.935098 -1.211422 -1.400513e+00   \n",
      "1998-01-02  1.015406  1.049596 -0.482037 -0.456681 -9.295647e-01   \n",
      "1998-01-03  1.102038  1.157046 -0.509495 -0.328518 -9.295647e-01   \n",
      "1998-01-04  0.322351  0.834697 -0.866453 -0.328518 -7.452807e-01   \n",
      "1998-01-05 -0.543968 -0.132352  0.012212  0.013251  3.637269e-16   \n",
      "...              ...       ...       ...       ...           ...   \n",
      "2001-06-29 -0.890495 -0.777051  0.643752  0.668309  7.904198e-01   \n",
      "2001-06-30 -0.890495 -0.454702  0.794773  0.839194  5.037557e-01   \n",
      "2001-07-01 -0.197440 -0.454702  0.273066  0.540146  6.061357e-01   \n",
      "2001-07-02 -1.063759 -1.099401  0.671211  0.540146  7.289918e-01   \n",
      "2001-07-03 -1.496918 -1.099401  0.684940  0.739511  6.675638e-01   \n",
      "\n",
      "                    RH85           U85           V85      HT85           T70  \\\n",
      "Date                                                                           \n",
      "1998-01-01 -1.821665e+00  3.465874e-01 -2.757654e-01  2.171370 -2.120681e+00   \n",
      "1998-01-02 -1.250812e+00 -5.796171e-01  1.297834e+00  1.699921 -2.094633e+00   \n",
      "1998-01-03  1.321889e-02 -2.956393e-01  1.405087e+00  0.999483 -1.287150e+00   \n",
      "1998-01-04  1.358801e+00 -5.643260e-01  1.138631e+00  0.406804 -7.401445e-01   \n",
      "1998-01-05 -1.810783e-15 -9.700882e-17 -2.232648e-16  0.000000  2.313515e-16   \n",
      "...                  ...           ...           ...       ...           ...   \n",
      "2001-06-29 -1.498819e-01 -3.065616e-01 -1.098501e-02  1.295822  4.129141e-02   \n",
      "2001-06-30  1.154925e+00 -4.878704e-01  1.498942e-01  0.810903 -2.191872e-01   \n",
      "2001-07-01  9.510486e-01 -7.501043e-02  1.582819e-02  0.635794  1.454829e-01   \n",
      "2001-07-02  3.394205e-01 -9.509727e-01 -6.963889e-02  0.757023  2.236265e-01   \n",
      "2001-07-03  8.287229e-01 -1.315775e+00 -3.746391e-01  1.403582  1.194350e-01   \n",
      "\n",
      "            ...           U50           V50      HT50            KI  \\\n",
      "Date        ...                                                       \n",
      "1998-01-01  ...  5.589683e-02 -2.784312e-01 -0.263699 -1.082313e+00   \n",
      "1998-01-02  ... -1.850338e-01  4.672686e-01 -0.137614  2.425325e-01   \n",
      "1998-01-03  ... -3.382572e-01  1.290300e+00 -0.326742  4.375862e-01   \n",
      "1998-01-04  ... -1.491055e-01  1.392489e+00 -0.515869  1.108875e+00   \n",
      "1998-01-05  ... -1.313970e-15 -6.132541e-17  0.000000 -6.299729e-16   \n",
      "...         ...           ...           ...       ...           ...   \n",
      "2001-06-29  ... -1.575592e-01 -2.383844e-01  0.681937  9.391530e-01   \n",
      "2001-06-30  ... -5.031044e-01  2.131783e-01  0.429768  1.334327e+00   \n",
      "2001-07-01  ... -8.296288e-01  5.017917e-01  0.492810  1.157005e+00   \n",
      "2001-07-02  ... -1.266051e+00  6.716456e-01  0.618895  9.543520e-01   \n",
      "2001-07-03  ... -1.503812e+00  2.711772e-01  0.997150  1.329261e+00   \n",
      "\n",
      "                      TT       SLP     Precp      T_SD    WSR_SD  month  \n",
      "Date                                                                     \n",
      "1998-01-01 -1.786384e+00  3.202663 -0.261979  1.795812  0.787244      1  \n",
      "1998-01-02 -7.604202e-01  2.139147 -0.261979 -0.613546 -0.559355      1  \n",
      "1998-01-03  3.764586e-01  1.365681 -0.261979 -0.925354  0.039145      1  \n",
      "1998-01-04  1.337722e+00  0.592215  1.353582 -2.214825 -0.778085      1  \n",
      "1998-01-05  3.940493e-15  0.000000  0.188514 -0.447411 -0.803654      1  \n",
      "...                  ...       ...       ...       ...       ...    ...  \n",
      "2001-06-29  6.999607e-01  0.495532 -0.223144 -0.617773 -0.192212      6  \n",
      "2001-06-30  1.143621e+00  0.205482 -0.122171 -0.229046 -0.407156      6  \n",
      "2001-07-01  1.014220e+00 -0.181251 -0.083336 -1.183509 -0.244536      7  \n",
      "2001-07-02  7.184465e-01 -0.084568  1.967185  0.037936 -0.834753      7  \n",
      "2001-07-03  8.154971e-01  0.495532 -0.261979  0.067925 -1.032378      7  \n",
      "\n",
      "[1267 rows x 26 columns]\n",
      "\n",
      "y : \n",
      "Date\n",
      "1998-01-01    0.0\n",
      "1998-01-02    0.0\n",
      "1998-01-03    0.0\n",
      "1998-01-04    0.0\n",
      "1998-01-05    0.0\n",
      "             ... \n",
      "2001-06-29    0.0\n",
      "2001-06-30    0.0\n",
      "2001-07-01    0.0\n",
      "2001-07-02    0.0\n",
      "2001-07-03    0.0\n",
      "Name: OZONE, Length: 1267, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def eda(all_df):\n",
    "    # データの追加,気温・風速の標準偏差\n",
    "    #1時間ごとの気温・風速を取得\n",
    "    T_data = all_df[['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23']]\n",
    "    WSR_data = all_df[['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23']]\n",
    "    # 行ごとの標準偏差を追加\n",
    "    all_df['T_SD'] = T_data.std(axis=1)\n",
    "    all_df['WSR_SD'] = WSR_data.std(axis=1)\n",
    "    \n",
    "    # ○○ヘクトパスカル面の風速を統一\n",
    "    # all_df['UV50'] = np.sqrt(all_df['U85']**2+ all_df['V85']**2)\n",
    "    # all_df['UV70'] = np.sqrt(all_df['U70']**2 + all_df['V70']**2)\n",
    "    # all_df['UV50'] = np.sqrt(all_df['U50']**2 + all_df['V50']**2)\n",
    "    \n",
    "    # 時系列(月)を特徴量に追加\n",
    "    # all_df['month'] =  all_df.index.month\n",
    "\n",
    "    # データの削除, T0~T23\n",
    "    all_df = all_df.drop(columns=['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23'])\n",
    "    # データの削除, WSR0~WSR23\n",
    "    all_df = all_df.drop(columns=['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23'])\n",
    "    \n",
    "    # データの削除, SLP_\n",
    "    all_df = all_df.drop(columns=['SLP_'])\n",
    "    \n",
    "    # ○○ヘクトパスカル面の風速の各方向を削除, U85,V85,U70,...\n",
    "    # all_df = all_df.drop(columns=['U85','V85','U70','V70','U50','V50'])\n",
    "    \n",
    "    return all_df\n",
    "\n",
    "\n",
    "# 特徴量の削除/追加\n",
    "all_df = eda(all_df)\n",
    "\n",
    "# trainとtestに分けなおす\n",
    "train_df = all_df[all_df['type'] == 'train']\n",
    "test_df = all_df[all_df['type'] == 'test']\n",
    "# train正解ラベル\n",
    "y = train_df['OZONE']\n",
    "\n",
    "# 学習に不要な特徴量を削除\n",
    "train_df = train_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "test_df = test_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "\n",
    "# 欠損値を平均値で補完\n",
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "\n",
    "# データ標準化(rightGBMのときはいらない)\n",
    "scaler = StandardScaler()\n",
    "train_df = pd.DataFrame(scaler.fit_transform(train_df), index = train_df.index, columns = train_df.columns)\n",
    "test_df = pd.DataFrame(scaler.transform(test_df), index = test_df.index, columns = test_df.columns)\n",
    "\n",
    "# 時系列(月)を特徴量に追加\n",
    "train_df['month'] =  train_df.index.month\n",
    "test_df['month'] =  test_df.index.month\n",
    "\n",
    "print(f'train_df : \\n{train_df}\\n')\n",
    "print(f'y : \\n{y}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8018f986-3fcc-4e65-b9c5-ef30fbf6f7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 改めてピアソン相関\\ncorr_matrix = train_df.corr()\\ny_corr = corr_matrix[y]\\n# 横棒グラフ\\nfig, ax = plt.subplots(figsize=(10, 10)) \\nsns.barplot(x=y_corr, y=y_corr.index, ax=ax) \\n#X,Y軸とグラフタイトル \\nax.set_xlabel(\"Correlation Coefficient\") \\nax.set_ylabel(\"Features\") \\nax.set_title(f\"Correlation with {y}\") \\n#表示 \\nplt.show()\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 改めてピアソン相関\n",
    "corr_matrix = train_df.corr()\n",
    "y_corr = corr_matrix[y]\n",
    "# 横棒グラフ\n",
    "fig, ax = plt.subplots(figsize=(10, 10)) \n",
    "sns.barplot(x=y_corr, y=y_corr.index, ax=ax) \n",
    "#X,Y軸とグラフタイトル \n",
    "ax.set_xlabel(\"Correlation Coefficient\") \n",
    "ax.set_ylabel(\"Features\") \n",
    "ax.set_title(f\"Correlation with {y}\") \n",
    "#表示 \n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11626f4f-6792-4c53-b5b1-2783f4c8b1eb",
   "metadata": {},
   "source": [
    "## 検証データ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c293299a-f8e0-4f3a-a606-e74c0cf57e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 訓練データの一部を分割し検証データを作成\n",
    "# 注意 :   \n",
    "# shuffleをTrueにするとランダムに分割されます。\n",
    "# この時、random_stateを定義していないとモデルの再現性が取れなくなるので、設定するよう心がけてください。\n",
    "# test_size=0.2とすることで訓練データの２割を検証データにしている\n",
    "\"\"\"\n",
    "X_train ,X_val ,y_train ,y_val = train_test_split(\n",
    "    train_df, y, \n",
    "    test_size=0.3, shuffle=True,random_state=0\n",
    "    )\n",
    "\"\"\"\n",
    "X_train ,y_train = train_df, y, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e04ad9-4c7e-477b-a6d3-e8d780532ac1",
   "metadata": {},
   "source": [
    "## モデルの作成と評価\n",
    "今回はロジェスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eba39fe1-1454-4fae-9470-47c839b9590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925808997632202\n",
      "0.925808997632202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# モデルを定義し学習\n",
    "model = LogisticRegression(max_iter=3000) \n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "\n",
    "# 訓練データに対しての予測を行い、正答率を算出\n",
    "y_pred = model.predict(X_train)\n",
    "print(accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffa375e5-614c-4ec5-8ca6-786c21eb1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータを予測\n",
    "test_pred = model.predict(test_df)\n",
    "\n",
    "# 行数で繰り返し予測値を代入\n",
    "for index, row in sample_sub.iterrows():\n",
    "    sample_sub.iloc[index,1] = np.where(test_pred[index]>=0.5, 1, 0)\n",
    "\n",
    "# 結果を保存\n",
    "# sample_sub.to_csv(\"../data/submission_Logistic3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9da5b8ea-871b-4fa9-a012-1cad352b51f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa35586-de7e-4dec-9087-2ec0d7350566",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27384/699222003.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_std\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;31m# not guaranteed that we use all of the available training data when we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m     \u001b[1;31m# use the first 'n_max_training_samples' samples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m     train_sizes_abs = _translate_train_sizes(train_sizes,\n\u001b[0m\u001b[0;32m   1385\u001b[0m                                              n_max_training_samples)\n\u001b[0;32m   1386\u001b[0m     \u001b[0mn_unique_ticks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_sizes_abs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_translate_train_sizes\u001b[1;34m(train_sizes, n_max_training_samples)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \"\"\"\n\u001b[0;32m   1460\u001b[0m     \u001b[0mtrain_sizes_abs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m     \u001b[0mn_ticks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_sizes_abs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m     \u001b[0mn_min_required_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sizes_abs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m     \u001b[0mn_max_required_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sizes_abs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = model, X = X_train,train_sizes=100 ,y = y_train, n_jobs=1)\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std  = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std  = np.std(test_scores, axis=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "# Traing score と Test score をプロット\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Validation score\")\n",
    "# 標準偏差の範囲を色付け\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, color=\"r\", alpha=0.2)\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, color=\"g\", alpha=0.2)\n",
    "# Y軸の範囲\n",
    "plt.ylim(0.7, 1.0)\n",
    "# 凡例の表示位置\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a68dc-b764-444d-af0c-0642253c1fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
