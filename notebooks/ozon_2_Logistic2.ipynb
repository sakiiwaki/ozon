{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb31b7c-60d9-4ab7-b721-4c7e37b79db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ozon_4_.ipynb\n",
      "../data/sample_submit.csv\n",
      "../data/smoto_submission_Logistic.csv\n",
      "../data/submission_ensemble.csv\n",
      "../data/submission_lightBGM.csv\n",
      "../data/submission_lightBGM2.csv\n",
      "../data/submission_lightBGM2_2.csv\n",
      "../data/submission_lightBGM3.csv\n",
      "../data/submission_Logistic.csv\n",
      "../data/submission_Logistic2.csv\n",
      "../data/submission_Logistic3.csv\n",
      "../data/test.tsv\n",
      "../data/train.tsv\n",
      "../data/.ipynb_checkpoints\\ozon_4_-checkpoint.ipynb\n",
      "../data/.ipynb_checkpoints\\sample_submit-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_ensemble-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic2-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\test-checkpoint.tsv\n",
      "../data/.ipynb_checkpoints\\train-checkpoint.tsv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../data/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24b6ef6-e994-4114-827e-b92232f296ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame._add_numeric_operations.<locals>.sum of               id  WSR0  WSR1  WSR2  WSR3  WSR4  WSR5  WSR6  WSR7  WSR8  ...  \\\n",
       "Date                                                                    ...   \n",
       "1998-04-05    94   0.4   0.5   2.1   2.2   2.5   2.4   2.1   2.9   3.6  ...   \n",
       "1998-04-11   100   0.0   0.6   0.4   0.3   0.1   0.3   0.2   1.4   2.6  ...   \n",
       "1998-04-20   109   1.8   0.3   0.1   0.1   0.1   0.2   0.2   0.7   0.9  ...   \n",
       "1998-04-23   112   0.5   0.1   0.1   0.1   0.1   0.2   0.3   0.8   1.2  ...   \n",
       "1998-04-25   114   3.1   2.4   2.4   3.0   3.4   3.4   3.9   4.5   5.5  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "2001-05-23  1225   0.8   0.3   0.2   0.4   0.3   1.4   0.9   1.5   1.9  ...   \n",
       "2001-06-15  1248   2.2   1.7   0.8   3.8   4.0   4.1   2.0   1.9   1.8  ...   \n",
       "2001-06-16  1249   0.4   0.4   0.1   0.1   0.0   0.4   0.5   0.6   1.3  ...   \n",
       "2001-06-18  1251   0.4   0.7   0.5   0.6   0.8   1.0   1.6   1.5   2.3  ...   \n",
       "2001-06-19  1252   0.7   0.7   0.1   0.1   0.2   0.4   0.4   0.4   0.8  ...   \n",
       "\n",
       "              U50    V50    HT50     KI     TT      SLP  SLP_  Precp  OZONE  \\\n",
       "Date                                                                          \n",
       "1998-04-05  20.91  -3.90  5755.0 -15.90  19.40  10140.0  20.0   0.00      1   \n",
       "1998-04-11  17.27 -12.27  5795.0 -12.60  24.20  10220.0  45.0   0.00      1   \n",
       "1998-04-20  20.36   2.61  5740.0  -3.50  30.60  10180.0  35.0   0.00      1   \n",
       "1998-04-23  16.78 -17.99  5680.0  -2.40  37.60  10195.0 -10.0   0.00      1   \n",
       "1998-04-25   9.22  -5.96  5790.0   7.10  35.40  10165.0 -25.0   0.00      1   \n",
       "...           ...    ...     ...    ...    ...      ...   ...    ...    ...   \n",
       "2001-05-23  17.58  -2.33  5795.0   3.40  35.70  10150.0  25.0   0.00      1   \n",
       "2001-06-15   7.11   7.01  5865.0  34.10  48.80  10150.0  50.0   3.33      1   \n",
       "2001-06-16   6.40  -5.96  5900.0   0.25  44.35  10185.0  35.0   0.00      1   \n",
       "2001-06-18  -4.00  -4.00  5900.0  -1.70  41.40  10180.0 -15.0   0.00      1   \n",
       "2001-06-19  -2.25  -3.93  5880.0  26.65  45.75  10165.0 -15.0   0.00      1   \n",
       "\n",
       "             type  \n",
       "Date               \n",
       "1998-04-05  train  \n",
       "1998-04-11  train  \n",
       "1998-04-20  train  \n",
       "1998-04-23  train  \n",
       "1998-04-25  train  \n",
       "...           ...  \n",
       "2001-05-23  train  \n",
       "2001-06-15  train  \n",
       "2001-06-16  train  \n",
       "2001-06-18  train  \n",
       "2001-06-19  train  \n",
       "\n",
       "[111 rows x 75 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_table('../data/train.tsv', index_col='Date', parse_dates=True)\n",
    "test_df = pd.read_table('../data/test.tsv', index_col='Date', parse_dates=True)\n",
    "sample_sub = pd.read_csv('../data/sample_submit.csv')\n",
    "\n",
    "# set type label\n",
    "train_df['type'] = 'train'\n",
    "test_df['type'] = 'test'\n",
    "\n",
    "# all data\n",
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# OZONEが高い日の数\n",
    "train_df[train_df[\"OZONE\"]==1.0].sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad5c3d-d96a-40f1-85d0-ea6b608575a8",
   "metadata": {},
   "source": [
    "## 学習する特徴量を作成\n",
    "#### __欠損値処理__\n",
    "→全部平均値で補完\n",
    "\n",
    "#### __特徴量の削除/追加__\n",
    "ピアソン相関から、  \n",
    "[削除]  \n",
    "- 時間ごとの気温\"T0\"~\"T23\"を消す\n",
    "- 時間ごとの風速\"WSR0\"~\"WSR24\"を消す\n",
    "- 海面気圧の前日からの変化\"SLP_\"を消す  \n",
    "\n",
    "[とりあえず追加]\n",
    "- 風速の標準偏差を追加する\n",
    "- 気温の標準偏差を追加する\n",
    "  \n",
    "計26個\n",
    "\n",
    "#### __データ変換__\n",
    "→とりあえず全部標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308334c6-5d7e-4bf0-b7c3-b1d0c72582b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df : \n",
      "        WSR_PK    WSR_AV      T_PK      T_AV           T85          RH85  \\\n",
      "0     1.015406  0.727247 -0.935098 -1.211422 -1.400513e+00 -1.821665e+00   \n",
      "1     1.015406  1.049596 -0.482037 -0.456681 -9.295647e-01 -1.250812e+00   \n",
      "2     1.102038  1.157046 -0.509495 -0.328518 -9.295647e-01  1.321889e-02   \n",
      "3     0.322351  0.834697 -0.866453 -0.328518 -7.452807e-01  1.358801e+00   \n",
      "4    -0.543968 -0.132352  0.012212  0.013251  3.637269e-16 -1.810783e-15   \n",
      "...        ...       ...       ...       ...           ...           ...   \n",
      "2307 -0.753449 -0.901346  1.437890  1.378840  1.440302e+00 -8.156491e-01   \n",
      "2308  0.026156 -0.325424  0.385941  0.255205  5.305264e-01 -1.812962e+00   \n",
      "2309 -0.543968 -0.908089  1.366817  1.191265  1.431000e+00 -1.760681e-01   \n",
      "2310 -0.613926 -0.785294  0.796935  0.644256 -9.037970e-02 -1.391275e-01   \n",
      "2311  0.208175 -0.357300  0.962090  0.726602  1.518748e+00 -1.137638e+00   \n",
      "\n",
      "               U85           V85      HT85           T70  ...           U50  \\\n",
      "0     3.465874e-01 -2.757654e-01  2.171370 -2.120681e+00  ...  5.589683e-02   \n",
      "1    -5.796171e-01  1.297834e+00  1.699921 -2.094633e+00  ... -1.850338e-01   \n",
      "2    -2.956393e-01  1.405087e+00  0.999483 -1.287150e+00  ... -3.382572e-01   \n",
      "3    -5.643260e-01  1.138631e+00  0.406804 -7.401445e-01  ... -1.491055e-01   \n",
      "4    -9.700882e-17 -2.232648e-16  0.000000  2.313515e-16  ... -1.313970e-15   \n",
      "...            ...           ...       ...           ...  ...           ...   \n",
      "2307 -1.399536e+00 -1.247504e+00  0.327905  2.313515e-16  ... -1.313970e-15   \n",
      "2308 -7.472010e-01 -4.359603e-01 -0.115759  1.243059e+00  ... -3.779785e-01   \n",
      "2309 -1.457056e+00 -1.011910e-01  0.710152  9.561417e-01  ... -1.828329e+00   \n",
      "2310  1.508916e-02 -2.488048e-01  0.120381 -1.291160e-01  ...  7.278859e-02   \n",
      "2311 -8.656574e-01  1.677580e-01  1.259947  1.427674e+00  ... -8.072178e-01   \n",
      "\n",
      "               V50      HT50            KI            TT       SLP     Precp  \\\n",
      "0    -2.784312e-01 -0.263699 -1.082313e+00 -1.786384e+00  3.202663 -0.261979   \n",
      "1     4.672686e-01 -0.137614  2.425325e-01 -7.604202e-01  2.139147 -0.261979   \n",
      "2     1.290300e+00 -0.326742  4.375862e-01  3.764586e-01  1.365681 -0.261979   \n",
      "3     1.392489e+00 -0.515869  1.108875e+00  1.337722e+00  0.592215  1.353582   \n",
      "4    -6.132541e-17  0.000000 -6.299729e-16  3.940493e-15  0.000000  0.188514   \n",
      "...            ...       ...           ...           ...       ...       ...   \n",
      "2307 -6.132541e-17  0.000000 -6.299729e-16  3.940493e-15 -0.746193 -0.261979   \n",
      "2308 -2.513542e-01  0.232787 -3.938566e-01 -6.474830e-01 -0.239911 -0.261979   \n",
      "2309 -5.984459e-01  1.109395  8.140586e-01  5.958840e-01 -0.450076 -0.261979   \n",
      "2310 -1.686203e-01 -0.020073  5.096528e-02 -1.250732e-01  0.081337 -0.261979   \n",
      "2311 -1.312307e+00  1.501489  2.191975e-01  4.337144e-01  0.309398 -0.261979   \n",
      "\n",
      "          T_SD    WSR_SD  month  \n",
      "0     1.795812  0.787244      1  \n",
      "1    -0.613546 -0.559355      1  \n",
      "2    -0.925354  0.039145      1  \n",
      "3    -2.214825 -0.778085      1  \n",
      "4    -0.447411 -0.803654      1  \n",
      "...        ...       ...    ...  \n",
      "2307  0.618612 -0.551892      8  \n",
      "2308  0.690721  0.283674      9  \n",
      "2309  0.800740 -0.088355      8  \n",
      "2310  0.691837 -0.220459      8  \n",
      "2311  0.746272  0.726458      5  \n",
      "\n",
      "[2312 rows x 26 columns]\n",
      "\n",
      "y : \n",
      "0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "2307    1.0\n",
      "2308    1.0\n",
      "2309    1.0\n",
      "2310    1.0\n",
      "2311    1.0\n",
      "Name: OZONE, Length: 2312, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def eda(all_df):\n",
    "    # データの追加,気温・風速の標準偏差\n",
    "    #1時間ごとの気温・風速を取得\n",
    "    T_data = all_df[['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23']]\n",
    "    WSR_data = all_df[['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23']]\n",
    "    # 行ごとの標準偏差を追加\n",
    "    all_df['T_SD'] = T_data.std(axis=1)\n",
    "    all_df['WSR_SD'] = WSR_data.std(axis=1)\n",
    "    \n",
    "    # ○○ヘクトパスカル面の風速を統一\n",
    "    # all_df['UV50'] = np.sqrt(all_df['U85']**2+ all_df['V85']**2)\n",
    "    # all_df['UV70'] = np.sqrt(all_df['U70']**2 + all_df['V70']**2)\n",
    "    # all_df['UV50'] = np.sqrt(all_df['U50']**2 + all_df['V50']**2)\n",
    "    \n",
    "    # 時系列(月)を特徴量に追加\n",
    "    # all_df['month'] =  all_df.index.month\n",
    "\n",
    "    # データの削除, T0~T23\n",
    "    all_df = all_df.drop(columns=['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23'])\n",
    "    # データの削除, WSR0~WSR23\n",
    "    all_df = all_df.drop(columns=['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23'])\n",
    "    \n",
    "    # データの削除, SLP_\n",
    "    all_df = all_df.drop(columns=['SLP_'])\n",
    "    \n",
    "    # ○○ヘクトパスカル面の風速の各方向を削除, U85,V85,U70,...\n",
    "    # all_df = all_df.drop(columns=['U85','V85','U70','V70','U50','V50'])\n",
    "    \n",
    "    return all_df\n",
    "\n",
    "\n",
    "# 特徴量の削除/追加\n",
    "all_df = eda(all_df)\n",
    "\n",
    "# trainとtestに分けなおす\n",
    "train_df = all_df[all_df['type'] == 'train']\n",
    "test_df = all_df[all_df['type'] == 'test']\n",
    "# train正解ラベル\n",
    "y = train_df['OZONE']\n",
    "\n",
    "# 学習に不要な特徴量を削除\n",
    "train_df = train_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "test_df = test_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "\n",
    "# 欠損値を平均値で補完\n",
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "\n",
    "# データ標準化(rightGBMのときはいらない)\n",
    "scaler = StandardScaler()\n",
    "train_df = pd.DataFrame(scaler.fit_transform(train_df), index = train_df.index, columns = train_df.columns)\n",
    "test_df = pd.DataFrame(scaler.transform(test_df), index = test_df.index, columns = test_df.columns)\n",
    "\n",
    "# 時系列(月)を特徴量に追加\n",
    "train_df['month'] =  train_df.index.month\n",
    "test_df['month'] =  test_df.index.month\n",
    "\n",
    "# オーバーサンプリング\n",
    "# SMOTEの初期化と適用\n",
    "smote = SMOTE(random_state=42)\n",
    "train_df, y = smote.fit_resample(train_df, y)\n",
    "\n",
    "print(f'train_df : \\n{train_df}\\n')\n",
    "print(f'y : \\n{y}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8018f986-3fcc-4e65-b9c5-ef30fbf6f7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 改めてピアソン相関\\ncorr_matrix = train_df.corr()\\ny_corr = corr_matrix[y]\\n# 横棒グラフ\\nfig, ax = plt.subplots(figsize=(10, 10)) \\nsns.barplot(x=y_corr, y=y_corr.index, ax=ax) \\n#X,Y軸とグラフタイトル \\nax.set_xlabel(\"Correlation Coefficient\") \\nax.set_ylabel(\"Features\") \\nax.set_title(f\"Correlation with {y}\") \\n#表示 \\nplt.show()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 改めてピアソン相関\n",
    "corr_matrix = train_df.corr()\n",
    "y_corr = corr_matrix[y]\n",
    "# 横棒グラフ\n",
    "fig, ax = plt.subplots(figsize=(10, 10)) \n",
    "sns.barplot(x=y_corr, y=y_corr.index, ax=ax) \n",
    "#X,Y軸とグラフタイトル \n",
    "ax.set_xlabel(\"Correlation Coefficient\") \n",
    "ax.set_ylabel(\"Features\") \n",
    "ax.set_title(f\"Correlation with {y}\") \n",
    "#表示 \n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11626f4f-6792-4c53-b5b1-2783f4c8b1eb",
   "metadata": {},
   "source": [
    "## 検証データ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c293299a-f8e0-4f3a-a606-e74c0cf57e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 訓練データの一部を分割し検証データを作成\n",
    "# 注意 :   \n",
    "# shuffleをTrueにするとランダムに分割されます。\n",
    "# この時、random_stateを定義していないとモデルの再現性が取れなくなるので、設定するよう心がけてください。\n",
    "# test_size=0.2とすることで訓練データの２割を検証データにしている\n",
    "\"\"\"\n",
    "X_train ,X_val ,y_train ,y_val = train_test_split(\n",
    "    train_df, y, \n",
    "    test_size=0.3, shuffle=True,random_state=0\n",
    "    )\n",
    "\"\"\"\n",
    "X_train ,y_train = train_df, y, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e04ad9-4c7e-477b-a6d3-e8d780532ac1",
   "metadata": {},
   "source": [
    "## モデルの作成と評価\n",
    "今回はロジェスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba39fe1-1454-4fae-9470-47c839b9590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8923010380622838\n",
      "0.8923010380622838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# モデルを定義し学習\n",
    "model = LogisticRegression(max_iter=3000) \n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "\n",
    "# 訓練データに対しての予測を行い、正答率を算出\n",
    "y_pred = model.predict(X_train)\n",
    "print(balanced_accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa375e5-614c-4ec5-8ca6-786c21eb1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータを予測\n",
    "test_pred = model.predict(test_df)\n",
    "\n",
    "# 行数で繰り返し予測値を代入\n",
    "for index, row in sample_sub.iterrows():\n",
    "    sample_sub.iloc[index,1] = np.where(test_pred[index]>=0.5, 1, 0)\n",
    "\n",
    "# 結果を保存\n",
    "# sample_sub.to_csv(\"../data/submission_Logistic3.csv\", index=False)\n",
    "sample_sub.to_csv(\"../data/sumoto_submission_Logistic2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da5b8ea-871b-4fa9-a012-1cad352b51f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
