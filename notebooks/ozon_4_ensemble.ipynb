{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88150ae3-5e90-40df-abcc-bc41a40a75d6",
   "metadata": {},
   "source": [
    "# アンサンブルモデル\n",
    "LightGBM、Random Forest、MLP、ロジスティック回帰とSVMの5つのモデルを用いて予測を行い、その結果の多数決をとり、最終的な予測を決定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530e7c73-1fbc-4fd3-a30d-348475402617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ozon_4_.ipynb\n",
      "../data/sample_submit.csv\n",
      "../data/submission_ensemble.csv\n",
      "../data/submission_lightBGM.csv\n",
      "../data/submission_lightBGM2.csv\n",
      "../data/submission_lightBGM2_2.csv\n",
      "../data/submission_lightBGM3.csv\n",
      "../data/submission_Logistic.csv\n",
      "../data/submission_Logistic2.csv\n",
      "../data/submission_Logistic3.csv\n",
      "../data/test.tsv\n",
      "../data/train.tsv\n",
      "../data/.ipynb_checkpoints\\ozon_4_-checkpoint.ipynb\n",
      "../data/.ipynb_checkpoints\\sample_submit-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_ensemble-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic2-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\test-checkpoint.tsv\n",
      "../data/.ipynb_checkpoints\\train-checkpoint.tsv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../data/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f57956-9ddc-413f-b300-f310dd74e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1267  0.0\n",
      "0     1268  0.0\n",
      "1     1269  0.0\n",
      "2     1270  1.0\n",
      "3     1271  1.0\n",
      "4     1272  1.0\n",
      "...    ...  ...\n",
      "1261  2529  1.0\n",
      "1262  2530  1.0\n",
      "1263  2531  0.0\n",
      "1264  2532  1.0\n",
      "1265  2533  0.0\n",
      "\n",
      "[1266 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('../data/train.tsv', index_col='Date', parse_dates=True)\n",
    "test_df = pd.read_table('../data/test.tsv', index_col='Date', parse_dates=True)\n",
    "sample_sub = pd.read_csv('../data/sample_submit.csv')\n",
    "print(sample_sub)\n",
    "\n",
    "# set type label\n",
    "train_df['type'] = 'train'\n",
    "test_df['type'] = 'test'\n",
    "\n",
    "# all data\n",
    "all_df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4be17-4c1a-437d-8c64-5bdf63d119c7",
   "metadata": {},
   "source": [
    "## 特徴量前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3814c1cc-09ea-4765-a6a8-78a646dcd988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df : \n",
      "            WSR_PK  WSR_AV  T_PK  T_AV        T85      RH85       U85  \\\n",
      "Date                                                                    \n",
      "1998-01-01     5.5     3.1  19.1  12.5   6.700000  0.110000  3.830000   \n",
      "1998-01-02     5.5     3.4  22.4  17.8   9.000000  0.250000 -0.410000   \n",
      "1998-01-03     5.6     3.5  22.2  18.7   9.000000  0.560000  0.890000   \n",
      "1998-01-04     4.7     3.2  19.6  18.7   9.900000  0.890000 -0.340000   \n",
      "1998-01-05     3.7     2.3  26.0  21.1  13.539776  0.556758  2.243384   \n",
      "...            ...     ...   ...   ...        ...       ...       ...   \n",
      "2001-06-29     3.3     1.7  30.6  25.7  17.400000  0.520000  0.840000   \n",
      "2001-06-30     3.3     2.0  31.7  26.9  16.000000  0.840000  0.010000   \n",
      "2001-07-01     4.1     2.0  27.9  24.8  16.500000  0.790000  1.900000   \n",
      "2001-07-02     3.1     1.4  30.8  24.8  17.100000  0.640000 -2.110000   \n",
      "2001-07-03     2.6     1.4  30.9  26.2  16.800000  0.760000 -3.780000   \n",
      "\n",
      "                 V85         HT85       T70  ...      RH50        U50  \\\n",
      "Date                                         ...                        \n",
      "1998-01-01   0.14000  1612.000000 -2.300000  ...  0.150000  10.670000   \n",
      "1998-01-02   9.53000  1594.500000 -2.200000  ...  0.480000   8.390000   \n",
      "1998-01-03  10.17000  1568.500000  0.900000  ...  0.600000   6.940000   \n",
      "1998-01-04   8.58000  1546.500000  3.000000  ...  0.490000   8.730000   \n",
      "1998-01-05   1.78555  1531.399585  5.841479  ...  0.281562  10.141031   \n",
      "...              ...          ...       ...  ...       ...        ...   \n",
      "2001-06-29   1.72000  1579.500000  6.000000  ...  0.450000   8.650000   \n",
      "2001-06-30   2.68000  1561.500000  5.000000  ...  0.650000   5.380000   \n",
      "2001-07-01   1.88000  1555.000000  6.400000  ...  0.220000   2.290000   \n",
      "2001-07-02   1.37000  1559.500000  6.700000  ...  0.390000  -1.840000   \n",
      "2001-07-03  -0.45000  1583.500000  6.300000  ...  0.910000  -4.090000   \n",
      "\n",
      "                  V50        HT50         KI         TT           SLP  Precp  \\\n",
      "Date                                                                           \n",
      "1998-01-01  -1.560000  5795.00000 -12.100000  17.900000  10330.000000   0.00   \n",
      "1998-01-02   3.840000  5805.00000  14.050000  29.000000  10275.000000   0.00   \n",
      "1998-01-03   9.800000  5790.00000  17.900000  41.300000  10235.000000   0.00   \n",
      "1998-01-04  10.540000  5775.00000  31.150000  51.700000  10195.000000   2.08   \n",
      "1998-01-05   0.456265  5815.91443   9.262858  37.227059  10164.373444   0.58   \n",
      "...               ...         ...        ...        ...           ...    ...   \n",
      "2001-06-29  -1.270000  5870.00000  27.800000  44.800000  10190.000000   0.05   \n",
      "2001-06-30   2.000000  5850.00000  35.600000  49.600000  10175.000000   0.18   \n",
      "2001-07-01   4.090000  5855.00000  32.100000  48.200000  10155.000000   0.23   \n",
      "2001-07-02   5.320000  5865.00000  28.100000  45.000000  10160.000000   2.87   \n",
      "2001-07-03   2.420000  5895.00000  35.500000  46.050000  10190.000000   0.00   \n",
      "\n",
      "                T_SD    WSR_SD  \n",
      "Date                            \n",
      "1998-01-01  5.513447  1.362542  \n",
      "1998-01-02  2.423197  0.878363  \n",
      "1998-01-03  2.023270  1.093558  \n",
      "1998-01-04  0.369390  0.799717  \n",
      "1998-01-05  2.636282  0.790524  \n",
      "...              ...       ...  \n",
      "2001-06-29  2.417775  1.010372  \n",
      "2001-06-30  2.916358  0.933087  \n",
      "2001-07-01  1.692160  0.991559  \n",
      "2001-07-02  3.258790  0.779342  \n",
      "2001-07-03  3.297254  0.708284  \n",
      "\n",
      "[1267 rows x 25 columns]\n",
      "\n",
      "y : \n",
      "Date\n",
      "1998-01-01    0.0\n",
      "1998-01-02    0.0\n",
      "1998-01-03    0.0\n",
      "1998-01-04    0.0\n",
      "1998-01-05    0.0\n",
      "             ... \n",
      "2001-06-29    0.0\n",
      "2001-06-30    0.0\n",
      "2001-07-01    0.0\n",
      "2001-07-02    0.0\n",
      "2001-07-03    0.0\n",
      "Name: OZONE, Length: 1267, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def eda(all_df):\n",
    "    # データの追加,気温・風速の標準偏差\n",
    "    #1時間ごとの気温・風速を取得\n",
    "    T_data = all_df[['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23']]\n",
    "    WSR_data = all_df[['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23']]\n",
    "    # 行ごとの標準偏差を追加\n",
    "    all_df['T_SD'] = T_data.std(axis=1)\n",
    "    all_df['WSR_SD'] = WSR_data.std(axis=1)\n",
    "    # データの削除, T0~T23\n",
    "    all_df = all_df.drop(columns=['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23'])\n",
    "    # データの削除, WSR0~WSR23\n",
    "    all_df = all_df.drop(columns=['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23'])\n",
    "    # データの削除, SLP_\n",
    "    all_df = all_df.drop(columns=['SLP_'])\n",
    "    return all_df\n",
    "\n",
    "# データ標準化(rightGBMのときはいらない)\n",
    "def standardscaler(train_df, test_df):\n",
    "    scaler = StandardScaler()\n",
    "    train_df_standard = pd.DataFrame(scaler.fit_transform(train_df), index = train_df.index, columns = train_df.columns)\n",
    "    test_df_standard = pd.DataFrame(scaler.transform(test_df), index = test_df.index, columns = test_df.columns)\n",
    "    return train_df_standard, test_df_standard\n",
    "\n",
    "# 特徴量の削除/追加\n",
    "all_df = eda(all_df)\n",
    "\n",
    "# trainとtestに分けなおす\n",
    "train_df = all_df[all_df['type'] == 'train']\n",
    "test_df = all_df[all_df['type'] == 'test']\n",
    "# train正解ラベル\n",
    "y = train_df['OZONE']\n",
    "\n",
    "# 学習に不要な特徴量を削除\n",
    "train_df = train_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "test_df = test_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "\n",
    "# 欠損値を平均値で補完\n",
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "print(f'train_df : \\n{train_df}\\n')\n",
    "print(f'y : \\n{y}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0640a52-ed07-4eb7-91cd-f93aecf4b194",
   "metadata": {},
   "source": [
    "### lightGBM\n",
    "#### +ハイパーパラメータチューニング(深さ、葉数、学習率)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f811312-aa2d-487a-a22a-dc8e6b152351",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5858cc-0b09-48a9-8692-ebe9c59305d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "[LightGBM] [Info] Number of positive: 95, number of negative: 918\n",
      "[LightGBM] [Info] Total Bins 4561\n",
      "[LightGBM] [Info] Number of data points in the train set: 1013, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093781 -> initscore=-2.268320\n",
      "[LightGBM] [Info] Start training from score -2.268320\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's binary_error: 0.0454097\tvalid_1's binary_error: 0.0472441\n",
      "---------- Start_rf ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_mlp ,y_val_mlp = y[trn_index], y[val_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "32/32 [==============================] - 0s 708us/step\n",
      "8/8 [==============================] - 0s 855us/step\n",
      "Fold : 1\n",
      "[LightGBM] [Info] Number of positive: 83, number of negative: 930\n",
      "[LightGBM] [Info] Total Bins 4490\n",
      "[LightGBM] [Info] Number of data points in the train set: 1013, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.081935 -> initscore=-2.416344\n",
      "[LightGBM] [Info] Start training from score -2.416344\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's binary_error: 0.0819348\tvalid_1's binary_error: 0.110236\n",
      "---------- Start_rf ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_mlp ,y_val_mlp = y[trn_index], y[val_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "32/32 [==============================] - 0s 901us/step\n",
      "8/8 [==============================] - 0s 855us/step\n",
      "Fold : 2\n",
      "[LightGBM] [Info] Number of positive: 86, number of negative: 928\n",
      "[LightGBM] [Info] Total Bins 4451\n",
      "[LightGBM] [Info] Number of data points in the train set: 1014, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.084813 -> initscore=-2.378684\n",
      "[LightGBM] [Info] Start training from score -2.378684\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0711462\n",
      "---------- Start_rf ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_mlp ,y_val_mlp = y[trn_index], y[val_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "32/32 [==============================] - 0s 676us/step\n",
      "8/8 [==============================] - 0s 855us/step\n",
      "Fold : 3\n",
      "[LightGBM] [Info] Number of positive: 91, number of negative: 923\n",
      "[LightGBM] [Info] Total Bins 4445\n",
      "[LightGBM] [Info] Number of data points in the train set: 1014, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.089744 -> initscore=-2.316770\n",
      "[LightGBM] [Info] Start training from score -2.316770\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0592885\n",
      "---------- Start_rf ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_mlp ,y_val_mlp = y[trn_index], y[val_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "32/32 [==============================] - 0s 708us/step\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "Fold : 4\n",
      "[LightGBM] [Info] Number of positive: 89, number of negative: 925\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 1014, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.087771 -> initscore=-2.341157\n",
      "[LightGBM] [Info] Start training from score -2.341157\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's binary_error: 0.00197239\tvalid_1's binary_error: 0.0592885\n",
      "---------- Start_rf ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train ,y_val = y[trn_index],y[val_index]\n",
      "C:\\Users\\saki\\AppData\\Local\\Temp\\ipykernel_12784\\3851638300.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_mlp ,y_val_mlp = y[trn_index], y[val_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "32/32 [==============================] - 0s 708us/step\n",
      "8/8 [==============================] - 0s 997us/step\n",
      "----------Result----------\n",
      "Train_acc : [0.9062191510365252, 0.926949654491609, 0.9418145956607495, 0.9378698224852071, 0.9408284023668639] , Ave : 0.9307363252081909\n",
      "Valid_acc : [0.937007874015748, 0.8937007874015748, 0.9051383399209486, 0.924901185770751, 0.9209486166007905] , Ave : 0.9163393607419625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\"\"\"\n",
    "# tensorflowの代わり\n",
    "def to_categorical(y, num_classes=np.amax(y)+1):\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\"\"\"\n",
    "# 乱数を固定\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "# KFold で学習させる\n",
    "cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# ハイパーパラメータを定義　\n",
    "lgb_params = {\n",
    "    \"objective\":\"binary\",\n",
    "    \"metric\": \"binary_error\",\n",
    "    \"force_row_wise\" : True,\n",
    "    \"seed\" : 0,\n",
    "    'learning_rate': 0.16394902178188858,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'num_leaves': 25,\n",
    "    'max_depth': 13,\n",
    "    }\n",
    "\n",
    "# indexをDateから普通のindexに直す(kholdが使えないため)、日付は消す\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# アンサンブルなモデルたち\n",
    "for i ,(trn_index, val_index) in enumerate(cv.split(train_df, y)):\n",
    "    \n",
    "    print(f'Fold : {i}')\n",
    "    X_train ,X_val = train_df.loc[trn_index], train_df.loc[val_index]\n",
    "    y_train ,y_val = y[trn_index],y[val_index]\n",
    "    \n",
    "    # *** LigthGBM Part ***\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_val, y_val)\n",
    "    \n",
    "    model_lgb = lgb.train(\n",
    "        params = lgb_params, \n",
    "        train_set = lgb_train,\n",
    "        valid_sets = [lgb_train, lgb_valid], \n",
    "        callbacks = [lgb.log_evaluation(period=0),lgb.early_stopping(10)],\n",
    "       )\n",
    "    \n",
    "    # *** RandomForest Part ***\n",
    "    print('-' *10 +' Start_rf ' +'-' *10)\n",
    "    model_rf = RandomForestClassifier(\n",
    "        random_state=0,max_depth=15,\n",
    "        min_samples_leaf=5,min_samples_split=5\n",
    "        )\n",
    "    model_rf.fit(\n",
    "        X_train, y_train\n",
    "        )\n",
    "\n",
    "    # 標準化\n",
    "    train_df, test_df = standardscaler(train_df, test_df)\n",
    "    X_train ,X_val = train_df.loc[trn_index], train_df.loc[val_index]\n",
    "    y_train ,y_val = y[trn_index],y[val_index]\n",
    "    \n",
    "    # *** MLP Part ***\n",
    "    print('-' *10 +' Start_mlp ' +'-' *10)\n",
    "    \n",
    "    # MLP用にLabel-EncodingをOne-Hot Encodingに変換\n",
    "    X_train_mlp ,X_val_mlp = train_df.loc[trn_index],train_df.loc[val_index]\n",
    "    y_train_mlp ,y_val_mlp = y[trn_index], y[val_index]\n",
    "    \n",
    "    \n",
    "    model_mlp = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(X_train_mlp.shape[1]),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    \"\"\"\n",
    "    # torchで書き換え、途中\n",
    "    model_mlp = nn.Sequential(\n",
    "        nn.Input(shape=X_train_mlp.shape[1]), \n",
    "        nn.Linear(X_train_mlp.shape[1], 32), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(32, 16), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(16, 16), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(16, 2),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "        loss_fn = model_mlp.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model_mlp.parameters(), lr=learning_rate)\n",
    "  \n",
    "    \"\"\"\n",
    "    early_stopping =  EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            patience=10,\n",
    "                            mode='auto'\n",
    "                        )\n",
    "\n",
    "    model_mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model_mlp.fit(\n",
    "        X_train_mlp, to_categorical(y_train_mlp),validation_data = (X_val_mlp,to_categorical(y_val_mlp)),\n",
    "        batch_size=256, epochs=300, verbose=False,callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "\n",
    "    # *** LogisticRegression Part ***\n",
    "    print('-' *10 +' Start_rogi ' +'-' *10)\n",
    "    model_rogi = LogisticRegression()\n",
    "    model_rogi.fit(\n",
    "        X_train_mlp, y_train\n",
    "        )\n",
    "    \n",
    "    # *** SVM Part ***\n",
    "    print('-' *10 +' Start_SVM ' +'-' *10)\n",
    "    model_svm = SVC(random_state=0)\n",
    "    model_svm.fit(\n",
    "        X_train_mlp, y_train\n",
    "        )\n",
    "    \n",
    "    # それぞれのモデルで予測し、正答率を算出\n",
    "    train_pred = np.zeros((len(y_train_mlp), 5))\n",
    "    \n",
    "    train_pred[:,0] = np.where(model_lgb.predict(X_train)>=0.5, 1, 0)\n",
    "    train_pred[:,1] = model_rf.predict(X_train)\n",
    "    train_pred[:,2] = np.argmax(model_mlp.predict(X_train_mlp),axis=1)\n",
    "    train_pred[:,3] = model_rogi.predict(X_train_mlp)\n",
    "    train_pred[:,4] = model_svm.predict(X_train_mlp)\n",
    "\n",
    "    train_acc = accuracy_score(\n",
    "        y_train, stats.mode(train_pred,axis=1)[0]\n",
    "        )\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    val_pred = np.zeros((len(y_val_mlp), 5))\n",
    "    \n",
    "    val_pred[:,0] = np.where(model_lgb.predict(X_val)>=0.5, 1, 0)\n",
    "    val_pred[:,1] = model_rf.predict(X_val)\n",
    "    val_pred[:,2] = np.argmax(model_mlp.predict(X_val_mlp),axis=1)\n",
    "    val_pred[:,3] = model_rogi.predict(X_val_mlp)\n",
    "    val_pred[:,4] = model_svm.predict(X_val_mlp)\n",
    "\n",
    "    val_acc = accuracy_score(\n",
    "        y_val, stats.mode(val_pred,axis=1)[0]\n",
    "        )\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    \n",
    "print('-'*10 + 'Result' +'-'*10)\n",
    "print(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\n",
    "print(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1de781-d045-456c-95d1-27a341664cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 639us/step\n",
      "0    25.0\n",
      "1     4.0\n",
      "2     0.0\n",
      "3    17.0\n",
      "4     0.0\n",
      "dtype: float64\n",
      "test_pred.sum : 4.0\n"
     ]
    }
   ],
   "source": [
    "# 予測結果をサブミットするファイル形式に変更\n",
    "test_pred = np.zeros((len(test_df), 5))\n",
    "\n",
    "test_pred[:,0] = np.where(model_lgb.predict(test_df)>=0.5, 1, 0)\n",
    "test_pred[:,1] = model_rf.predict(test_df)\n",
    "test_pred[:,2] = np.argmax(model_mlp.predict(test_df),axis=1)\n",
    "test_pred[:,3] = model_rogi.predict(test_df)\n",
    "test_pred[:,4] = model_svm.predict(test_df)\n",
    "\n",
    "\n",
    "# 提出ファイルを出力 \n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "print(test_pred.sum())\n",
    "test_pred = test_pred.mode(axis=1).values\n",
    "\n",
    "for index, row in sample_sub.iterrows():\n",
    "    sample_sub.iloc[index,1] = test_pred[index]\n",
    "\n",
    "\n",
    "print(f'test_pred.sum : {test_pred.sum()}')\n",
    "# 結果を保存\n",
    "sample_sub.to_csv(\"../data/submission_ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970b5a5-5390-4103-ab42-d468feb18f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
