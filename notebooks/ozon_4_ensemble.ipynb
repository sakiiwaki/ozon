{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88150ae3-5e90-40df-abcc-bc41a40a75d6",
   "metadata": {},
   "source": [
    "# アンサンブルモデル\n",
    "LightGBM、Random Forest、MLP、ロジスティック回帰とSVMの5つのモデルを用いて予測を行い、その結果の多数決をとり、最終的な予測を決定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530e7c73-1fbc-4fd3-a30d-348475402617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ozon_4_.ipynb\n",
      "../data/sample_submit.csv\n",
      "../data/smoto_submission_lightBGM.csv\n",
      "../data/smoto_submission_Logistic.csv\n",
      "../data/submission_ensemble.csv\n",
      "../data/submission_lightBGM.csv\n",
      "../data/submission_lightBGM2.csv\n",
      "../data/submission_lightBGM2_2.csv\n",
      "../data/submission_lightBGM3.csv\n",
      "../data/submission_Logistic.csv\n",
      "../data/submission_Logistic2.csv\n",
      "../data/submission_Logistic3.csv\n",
      "../data/sumoto_submission_Logistic2.csv\n",
      "../data/test.tsv\n",
      "../data/train.tsv\n",
      "../data/.ipynb_checkpoints\\ozon_4_-checkpoint.ipynb\n",
      "../data/.ipynb_checkpoints\\sample_submit-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_ensemble-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic2-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic3-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\test-checkpoint.tsv\n",
      "../data/.ipynb_checkpoints\\train-checkpoint.tsv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../data/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f57956-9ddc-413f-b300-f310dd74e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1267  0.0\n",
      "0     1268  0.0\n",
      "1     1269  0.0\n",
      "2     1270  1.0\n",
      "3     1271  1.0\n",
      "4     1272  1.0\n",
      "...    ...  ...\n",
      "1261  2529  1.0\n",
      "1262  2530  1.0\n",
      "1263  2531  0.0\n",
      "1264  2532  1.0\n",
      "1265  2533  0.0\n",
      "\n",
      "[1266 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('../data/train.tsv', index_col='Date', parse_dates=True)\n",
    "test_df = pd.read_table('../data/test.tsv', index_col='Date', parse_dates=True)\n",
    "sample_sub = pd.read_csv('../data/sample_submit.csv')\n",
    "print(sample_sub)\n",
    "\n",
    "# set type label\n",
    "train_df['type'] = 'train'\n",
    "test_df['type'] = 'test'\n",
    "\n",
    "# all data\n",
    "all_df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4be17-4c1a-437d-8c64-5bdf63d119c7",
   "metadata": {},
   "source": [
    "## 特徴量前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3814c1cc-09ea-4765-a6a8-78a646dcd988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df : \n",
      "        WSR_PK    WSR_AV       T_PK       T_AV        T85      RH85       U85  \\\n",
      "0     5.500000  3.100000  19.100000  12.500000   6.700000  0.110000  3.830000   \n",
      "1     5.500000  3.400000  22.400000  17.800000   9.000000  0.250000 -0.410000   \n",
      "2     5.600000  3.500000  22.200000  18.700000   9.000000  0.560000  0.890000   \n",
      "3     4.700000  3.200000  19.600000  18.700000   9.900000  0.890000 -0.340000   \n",
      "4     3.700000  2.300000  26.000000  21.100000  13.539776  0.556758  2.243384   \n",
      "...        ...       ...        ...        ...        ...       ...       ...   \n",
      "2307  3.505226  1.610452  36.138711  30.464840  20.354388  0.352019 -3.826904   \n",
      "2308  3.269236  1.659642  29.475985  23.469140  14.388468  0.244791  0.503609   \n",
      "2309  3.491874  1.552032  34.565858  28.721953  20.086174  0.573414 -4.884639   \n",
      "2310  4.203903  1.911575  28.646299  22.753970  12.996231  0.503082  3.625464   \n",
      "2311  4.568205  2.090649  32.918703  26.109351  20.956983  0.277756 -1.719442   \n",
      "\n",
      "            V85         HT85        T70  ...      RH50        U50        V50  \\\n",
      "0      0.140000  1612.000000  -2.300000  ...  0.150000  10.670000  -1.560000   \n",
      "1      9.530000  1594.500000  -2.200000  ...  0.480000   8.390000   3.840000   \n",
      "2     10.170000  1568.500000   0.900000  ...  0.600000   6.940000   9.800000   \n",
      "3      8.580000  1546.500000   3.000000  ...  0.490000   8.730000  10.540000   \n",
      "4      1.785550  1531.399585   5.841479  ...  0.281562  10.141031   0.456265   \n",
      "...         ...          ...        ...  ...       ...        ...        ...   \n",
      "2307  -5.743226  1541.768390   5.813182  ...  0.271029  10.267441   0.338880   \n",
      "2308  -0.193778  1531.457947   8.955887  ...  0.281562  10.141031   0.456265   \n",
      "2309   0.890345  1556.979684   9.330079  ...  0.162844  -6.388337  -1.947022   \n",
      "2310  -4.504670  1542.518843   4.657739  ...  0.241534  13.576985 -13.105639   \n",
      "2311   2.786597  1578.168326  11.322443  ...  0.404115   2.502082  -9.046835   \n",
      "\n",
      "             HT50         KI         TT           SLP    Precp      T_SD  \\\n",
      "0     5795.000000 -12.100000  17.900000  10330.000000  0.00000  5.513447   \n",
      "1     5805.000000  14.050000  29.000000  10275.000000  0.00000  2.423197   \n",
      "2     5790.000000  17.900000  41.300000  10235.000000  0.00000  2.023270   \n",
      "3     5775.000000  31.150000  51.700000  10195.000000  2.08000  0.369390   \n",
      "4     5815.914430   9.262858  37.227059  10164.373444  0.58000  2.636282   \n",
      "...           ...        ...        ...           ...      ...       ...   \n",
      "2307  5816.389222   9.322283  37.502612  10125.522579  0.00000  3.985261   \n",
      "2308  5815.914430   9.262858  37.227059  10170.549669  0.00000  4.178798   \n",
      "2309  5902.601578  24.160158  44.818679  10141.097633  0.13268  3.822693   \n",
      "2310  5802.691790  14.571265  41.189098  10172.691790  0.00000  4.142592   \n",
      "2311  5935.000000  13.589411  41.919456  10180.374058  0.00000  4.167304   \n",
      "\n",
      "        WSR_SD  \n",
      "0     1.362542  \n",
      "1     0.878363  \n",
      "2     1.093558  \n",
      "3     0.799717  \n",
      "4     0.790524  \n",
      "...        ...  \n",
      "2307  0.897198  \n",
      "2308  0.870811  \n",
      "2309  0.973340  \n",
      "2310  1.281189  \n",
      "2311  1.340686  \n",
      "\n",
      "[2312 rows x 25 columns]\n",
      "\n",
      "y : \n",
      "0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "2307    1.0\n",
      "2308    1.0\n",
      "2309    1.0\n",
      "2310    1.0\n",
      "2311    1.0\n",
      "Name: OZONE, Length: 2312, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def eda(all_df):\n",
    "    # データの追加,気温・風速の標準偏差\n",
    "    #1時間ごとの気温・風速を取得\n",
    "    T_data = all_df[['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23']]\n",
    "    WSR_data = all_df[['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23']]\n",
    "    # 行ごとの標準偏差を追加\n",
    "    all_df['T_SD'] = T_data.std(axis=1)\n",
    "    all_df['WSR_SD'] = WSR_data.std(axis=1)\n",
    "    # データの削除, T0~T23\n",
    "    all_df = all_df.drop(columns=['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23'])\n",
    "    # データの削除, WSR0~WSR23\n",
    "    all_df = all_df.drop(columns=['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23'])\n",
    "    # データの削除, SLP_\n",
    "    all_df = all_df.drop(columns=['SLP_'])\n",
    "    return all_df\n",
    "\n",
    "# データ標準化(rightGBMのときはいらない)\n",
    "def standardscaler(train_df, test_df):\n",
    "    scaler = StandardScaler()\n",
    "    train_df_standard = pd.DataFrame(scaler.fit_transform(train_df), index = train_df.index, columns = train_df.columns)\n",
    "    test_df_standard = pd.DataFrame(scaler.transform(test_df), index = test_df.index, columns = test_df.columns)\n",
    "    return train_df_standard, test_df_standard\n",
    "\n",
    "# 特徴量の削除/追加\n",
    "all_df = eda(all_df)\n",
    "\n",
    "# trainとtestに分けなおす\n",
    "train_df = all_df[all_df['type'] == 'train']\n",
    "test_df = all_df[all_df['type'] == 'test']\n",
    "# train正解ラベル\n",
    "y = train_df['OZONE']\n",
    "\n",
    "# 学習に不要な特徴量を削除\n",
    "train_df = train_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "test_df = test_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "\n",
    "# 欠損値を平均値で補完\n",
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "# オーバーサンプリング\n",
    "# SMOTEの初期化と適用\n",
    "smote = SMOTE(random_state=42)\n",
    "train_df, y = smote.fit_resample(train_df, y)\n",
    "\n",
    "print(f'train_df : \\n{train_df}\\n')\n",
    "print(f'y : \\n{y}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0640a52-ed07-4eb7-91cd-f93aecf4b194",
   "metadata": {},
   "source": [
    "### lightGBM\n",
    "#### +ハイパーパラメータチューニング(深さ、葉数、学習率)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5858cc-0b09-48a9-8692-ebe9c59305d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0388769\n",
      "---------- Start_rf ----------\n",
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "58/58 [==============================] - 0s 647us/step\n",
      "15/15 [==============================] - 0s 712us/step\n",
      "Fold : 1\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's binary_error: 0.0638183\tvalid_1's binary_error: 0.0647948\n",
      "---------- Start_rf ----------\n",
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "58/58 [==============================] - 0s 595us/step\n",
      "15/15 [==============================] - 0s 784us/step\n",
      "Fold : 2\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's binary_error: 0.0108108\tvalid_1's binary_error: 0.0519481\n",
      "---------- Start_rf ----------\n",
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "58/58 [==============================] - 0s 630us/step\n",
      "15/15 [==============================] - 0s 784us/step\n",
      "Fold : 3\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's binary_error: 0.0118919\tvalid_1's binary_error: 0.0541126\n",
      "---------- Start_rf ----------\n",
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "58/58 [==============================] - 0s 630us/step\n",
      "15/15 [==============================] - 0s 784us/step\n",
      "Fold : 4\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's binary_error: 0.00108108\tvalid_1's binary_error: 0.034632\n",
      "---------- Start_rf ----------\n",
      "---------- Start_mlp ----------\n",
      "---------- Start_rogi ----------\n",
      "---------- Start_SVM ----------\n",
      "58/58 [==============================] - 0s 595us/step\n",
      "15/15 [==============================] - 0s 641us/step\n",
      "----------Result----------\n",
      "Train_acc : [0.9421308815575987, 0.961060032449973, 0.9778378378378378, 0.98, 0.9756756756756757] , Ave : 0.967340885504217\n",
      "Valid_acc : [0.91792656587473, 0.9438444924406048, 0.9437229437229437, 0.9523809523809523, 0.9415584415584416] , Ave : 0.9398866791955346\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\"\"\"\n",
    "# tensorflowの代わり\n",
    "def to_categorical(y, num_classes=np.amax(y)+1):\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\"\"\"\n",
    "# 乱数を固定\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "# KFold で学習させる\n",
    "cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# ハイパーパラメータを定義　\n",
    "lgb_params = {\n",
    "    \"objective\":\"binary\",\n",
    "    \"metric\": \"binary_error\",\n",
    "    \"force_row_wise\" : True,\n",
    "    \"seed\" : 0,\n",
    "    'learning_rate': 0.09944437508200545,\n",
    "    # 'min_data_in_leaf': 5,\n",
    "    'num_leaves': 28,\n",
    "    'max_depth': 9,\n",
    "    'verbose': -1,\n",
    "    }\n",
    "\n",
    "# indexをDateから普通のindexに直す(kholdが使えないため)、日付は消す\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# アンサンブルなモデルたち\n",
    "for i ,(trn_index, val_index) in enumerate(cv.split(train_df, y)):\n",
    "    \n",
    "    print(f'Fold : {i}')\n",
    "    X_train ,X_val = train_df.loc[trn_index], train_df.loc[val_index]\n",
    "    y_train ,y_val = y[trn_index],y[val_index]\n",
    "    \n",
    "    # *** LigthGBM Part ***\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_val, y_val)\n",
    "    \n",
    "    model_lgb = lgb.train(\n",
    "        params = lgb_params, \n",
    "        train_set = lgb_train,\n",
    "        valid_sets = [lgb_train, lgb_valid], \n",
    "        callbacks = [lgb.log_evaluation(period=0),lgb.early_stopping(10)],\n",
    "       )\n",
    "    \n",
    "    # *** RandomForest Part ***\n",
    "    print('-' *10 +' Start_rf ' +'-' *10)\n",
    "    model_rf = RandomForestClassifier(\n",
    "        random_state=0,max_depth=15,\n",
    "        min_samples_leaf=5,min_samples_split=5\n",
    "        )\n",
    "    model_rf.fit(\n",
    "        X_train, y_train\n",
    "        )\n",
    "\n",
    "    # 標準化\n",
    "    train_df, test_df = standardscaler(train_df, test_df)\n",
    "    X_train ,X_val = train_df.loc[trn_index], train_df.loc[val_index]\n",
    "    y_train ,y_val = y[trn_index],y[val_index]\n",
    "    \n",
    "    # *** MLP Part ***\n",
    "    print('-' *10 +' Start_mlp ' +'-' *10)\n",
    "    \n",
    "    # MLP用にLabel-EncodingをOne-Hot Encodingに変換\n",
    "    X_train_mlp ,X_val_mlp = train_df.loc[trn_index],train_df.loc[val_index]\n",
    "    y_train_mlp ,y_val_mlp = y[trn_index], y[val_index]\n",
    "    \n",
    "    \n",
    "    model_mlp = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(X_train_mlp.shape[1]),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    \"\"\"\n",
    "    # torchで書き換え、途中\n",
    "    model_mlp = nn.Sequential(\n",
    "        nn.Input(shape=X_train_mlp.shape[1]), \n",
    "        nn.Linear(X_train_mlp.shape[1], 32), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(32, 16), \n",
    "        nn.ReLU(), \n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(16, 16), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(16, 2),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "        loss_fn = model_mlp.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model_mlp.parameters(), lr=learning_rate)\n",
    "  \n",
    "    \"\"\"\n",
    "    early_stopping =  EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            patience=10,\n",
    "                            mode='auto'\n",
    "                        )\n",
    "\n",
    "    model_mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model_mlp.fit(\n",
    "        X_train_mlp, to_categorical(y_train_mlp),validation_data = (X_val_mlp,to_categorical(y_val_mlp)),\n",
    "        batch_size=256, epochs=300, verbose=False,callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "\n",
    "    # *** LogisticRegression Part ***\n",
    "    print('-' *10 +' Start_rogi ' +'-' *10)\n",
    "    model_rogi = LogisticRegression()\n",
    "    model_rogi.fit(\n",
    "        X_train_mlp, y_train\n",
    "        )\n",
    "    \n",
    "    # *** SVM Part ***\n",
    "    print('-' *10 +' Start_SVM ' +'-' *10)\n",
    "    model_svm = SVC(random_state=0)\n",
    "    model_svm.fit(\n",
    "        X_train_mlp, y_train\n",
    "        )\n",
    "    \n",
    "    # それぞれのモデルで予測し、正答率を算出\n",
    "    train_pred = np.zeros((len(y_train_mlp), 5))\n",
    "    \n",
    "    train_pred[:,0] = np.where(model_lgb.predict(X_train)>=0.5, 1, 0)\n",
    "    train_pred[:,1] = model_rf.predict(X_train)\n",
    "    train_pred[:,2] = np.argmax(model_mlp.predict(X_train_mlp),axis=1)\n",
    "    train_pred[:,3] = model_rogi.predict(X_train_mlp)\n",
    "    train_pred[:,4] = model_svm.predict(X_train_mlp)\n",
    "\n",
    "    train_acc = accuracy_score(\n",
    "        y_train, stats.mode(train_pred,axis=1)[0]\n",
    "        )\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    val_pred = np.zeros((len(y_val_mlp), 5))\n",
    "    \n",
    "    val_pred[:,0] = np.where(model_lgb.predict(X_val)>=0.5, 1, 0)\n",
    "    val_pred[:,1] = model_rf.predict(X_val)\n",
    "    val_pred[:,2] = np.argmax(model_mlp.predict(X_val_mlp),axis=1)\n",
    "    val_pred[:,3] = model_rogi.predict(X_val_mlp)\n",
    "    val_pred[:,4] = model_svm.predict(X_val_mlp)\n",
    "\n",
    "    val_acc = accuracy_score(\n",
    "        y_val, stats.mode(val_pred,axis=1)[0]\n",
    "        )\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    \n",
    "print('-'*10 + 'Result' +'-'*10)\n",
    "print(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\n",
    "print(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1de781-d045-456c-95d1-27a341664cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 665us/step\n",
      "0     93.0\n",
      "1     98.0\n",
      "2     68.0\n",
      "3    192.0\n",
      "4    147.0\n",
      "dtype: float64\n",
      "test_pred.sum : 103.0\n"
     ]
    }
   ],
   "source": [
    "# 予測結果をサブミットするファイル形式に変更\n",
    "test_pred = np.zeros((len(test_df), 5))\n",
    "\n",
    "test_pred[:,0] = np.where(model_lgb.predict(test_df)>=0.5, 1, 0)\n",
    "test_pred[:,1] = model_rf.predict(test_df)\n",
    "test_pred[:,2] = np.argmax(model_mlp.predict(test_df),axis=1)\n",
    "test_pred[:,3] = model_rogi.predict(test_df)\n",
    "test_pred[:,4] = model_svm.predict(test_df)\n",
    "\n",
    "\n",
    "# 提出ファイルを出力 \n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "print(test_pred.sum())\n",
    "test_pred = test_pred.mode(axis=1).values\n",
    "\n",
    "for index, row in sample_sub.iterrows():\n",
    "    sample_sub.iloc[index,1] = test_pred[index]\n",
    "\n",
    "\n",
    "print(f'test_pred.sum : {test_pred.sum()}')\n",
    "# 結果を保存\n",
    "# sample_sub.to_csv(\"../data/submission_ensemble.csv\", index=False)\n",
    "sample_sub.to_csv(\"../data/smoto_submission_ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970b5a5-5390-4103-ab42-d468feb18f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
