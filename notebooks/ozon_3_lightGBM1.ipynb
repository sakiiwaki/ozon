{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05142ea6-485f-421d-b852-6624b2e403db",
   "metadata": {},
   "source": [
    "# LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530e7c73-1fbc-4fd3-a30d-348475402617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/sample_submit.csv\n",
      "../data/submission_lightBGM.csv\n",
      "../data/submission_Logistic.csv\n",
      "../data/test.tsv\n",
      "../data/train.tsv\n",
      "../data/.ipynb_checkpoints\\sample_submit-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_lightBGM-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\submission_Logistic-checkpoint.csv\n",
      "../data/.ipynb_checkpoints\\test-checkpoint.tsv\n",
      "../data/.ipynb_checkpoints\\train-checkpoint.tsv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../data/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f57956-9ddc-413f-b300-f310dd74e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1267  0.0\n",
      "0     1268  0.0\n",
      "1     1269  0.0\n",
      "2     1270  1.0\n",
      "3     1271  1.0\n",
      "4     1272  1.0\n",
      "...    ...  ...\n",
      "1261  2529  1.0\n",
      "1262  2530  1.0\n",
      "1263  2531  0.0\n",
      "1264  2532  1.0\n",
      "1265  2533  0.0\n",
      "\n",
      "[1266 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_table('../data/train.tsv', index_col='Date', parse_dates=True)\n",
    "test_df = pd.read_table('../data/test.tsv', index_col='Date', parse_dates=True)\n",
    "sample_sub = pd.read_csv('../data/sample_submit.csv')\n",
    "print(sample_sub)\n",
    "\n",
    "# set type label\n",
    "train_df['type'] = 'train'\n",
    "test_df['type'] = 'test'\n",
    "\n",
    "# all data\n",
    "all_df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4be17-4c1a-437d-8c64-5bdf63d119c7",
   "metadata": {},
   "source": [
    "## 特徴量前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3814c1cc-09ea-4765-a6a8-78a646dcd988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df : \n",
      "            WSR0  WSR1  WSR2  WSR3  WSR4  WSR5  WSR6  WSR7  WSR8  WSR9  ...  \\\n",
      "Date                                                                    ...   \n",
      "1998-01-01   0.8   1.8   2.4   2.1   2.0   2.1   1.5   1.7   1.9   2.3  ...   \n",
      "1998-01-02   2.8   3.2   3.3   2.7   3.3   3.2   2.9   2.8   3.1   3.4  ...   \n",
      "1998-01-03   2.9   2.8   2.6   2.1   2.2   2.5   2.5   2.7   2.2   2.5  ...   \n",
      "1998-01-04   4.7   3.8   3.7   3.8   2.9   3.1   2.8   2.5   2.4   3.1  ...   \n",
      "1998-01-05   2.6   2.1   1.6   1.4   0.9   1.5   1.2   1.4   1.3   1.4  ...   \n",
      "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "2001-06-29   0.8   0.9   0.3   0.3   0.7   0.8   1.3   1.9   3.0   3.0  ...   \n",
      "2001-06-30   1.0   0.7   0.8   0.6   0.9   1.2   1.3   1.7   2.8   2.9  ...   \n",
      "2001-07-01   0.7   1.0   1.4   0.9   1.0   1.2   1.4   1.4   1.1   2.0  ...   \n",
      "2001-07-02   1.7   1.7   1.1   1.3   1.0   1.0   1.2   1.8   1.8   1.8  ...   \n",
      "2001-07-03   0.5   0.5   0.8   0.6   0.8   0.8   1.0   1.5   1.8   1.8  ...   \n",
      "\n",
      "                RH50        U50        V50        HT50         KI         TT  \\\n",
      "Date                                                                           \n",
      "1998-01-01  0.150000  10.670000  -1.560000  5795.00000 -12.100000  17.900000   \n",
      "1998-01-02  0.480000   8.390000   3.840000  5805.00000  14.050000  29.000000   \n",
      "1998-01-03  0.600000   6.940000   9.800000  5790.00000  17.900000  41.300000   \n",
      "1998-01-04  0.490000   8.730000  10.540000  5775.00000  31.150000  51.700000   \n",
      "1998-01-05  0.281562  10.141031   0.456265  5815.91443   9.262858  37.227059   \n",
      "...              ...        ...        ...         ...        ...        ...   \n",
      "2001-06-29  0.450000   8.650000  -1.270000  5870.00000  27.800000  44.800000   \n",
      "2001-06-30  0.650000   5.380000   2.000000  5850.00000  35.600000  49.600000   \n",
      "2001-07-01  0.220000   2.290000   4.090000  5855.00000  32.100000  48.200000   \n",
      "2001-07-02  0.390000  -1.840000   5.320000  5865.00000  28.100000  45.000000   \n",
      "2001-07-03  0.910000  -4.090000   2.420000  5895.00000  35.500000  46.050000   \n",
      "\n",
      "                     SLP  Precp      T_SD    WSR_SD  \n",
      "Date                                                 \n",
      "1998-01-01  10330.000000   0.00  5.513447  1.362542  \n",
      "1998-01-02  10275.000000   0.00  2.423197  0.878363  \n",
      "1998-01-03  10235.000000   0.00  2.023270  1.093558  \n",
      "1998-01-04  10195.000000   2.08  0.369390  0.799717  \n",
      "1998-01-05  10164.373444   0.58  2.636282  0.790524  \n",
      "...                  ...    ...       ...       ...  \n",
      "2001-06-29  10190.000000   0.05  2.417775  1.010372  \n",
      "2001-06-30  10175.000000   0.18  2.916358  0.933087  \n",
      "2001-07-01  10155.000000   0.23  1.692160  0.991559  \n",
      "2001-07-02  10160.000000   2.87  3.258790  0.779342  \n",
      "2001-07-03  10190.000000   0.00  3.297254  0.708284  \n",
      "\n",
      "[1267 rows x 73 columns]\n",
      "\n",
      "y : \n",
      "Date\n",
      "1998-01-01    0.0\n",
      "1998-01-02    0.0\n",
      "1998-01-03    0.0\n",
      "1998-01-04    0.0\n",
      "1998-01-05    0.0\n",
      "             ... \n",
      "2001-06-29    0.0\n",
      "2001-06-30    0.0\n",
      "2001-07-01    0.0\n",
      "2001-07-02    0.0\n",
      "2001-07-03    0.0\n",
      "Name: OZONE, Length: 1267, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def eda(all_df):\n",
    "    # データの追加,気温・風速の標準偏差\n",
    "    #1時間ごとの気温・風速を取得\n",
    "    T_data = all_df[['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23']]\n",
    "    WSR_data = all_df[['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23']]\n",
    "    # 行ごとの標準偏差を追加\n",
    "    all_df['T_SD'] = T_data.std(axis=1)\n",
    "    all_df['WSR_SD'] = WSR_data.std(axis=1)\n",
    "    \"\"\"\n",
    "    # データの削除, T0~T23\n",
    "    all_df = all_df.drop(columns=['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19', 'T20', 'T21', 'T22', 'T23'])\n",
    "    # データの削除, WSR0~WSR23\n",
    "    all_df = all_df.drop(columns=['WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9', 'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19', 'WSR20', 'WSR21', 'WSR22', 'WSR23'])\n",
    "    \"\"\"\n",
    "    # データの削除, SLP_\n",
    "    all_df = all_df.drop(columns=['SLP_'])\n",
    "    return all_df\n",
    "\n",
    "\n",
    "# 特徴量の削除/追加\n",
    "all_df = eda(all_df)\n",
    "\n",
    "# trainとtestに分けなおす\n",
    "train_df = all_df[all_df['type'] == 'train']\n",
    "test_df = all_df[all_df['type'] == 'test']\n",
    "# train正解ラベル\n",
    "y = train_df['OZONE']\n",
    "\n",
    "# 学習に不要な特徴量を削除\n",
    "train_df = train_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "test_df = test_df.drop(columns=['id', 'OZONE', 'type'])\n",
    "\n",
    "# 欠損値を平均値で補完\n",
    "train_df = train_df.fillna(train_df.mean())\n",
    "test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# データ標準化(rightGBMのときはいらない)\n",
    "scaler = StandardScaler()\n",
    "train_df = pd.DataFrame(scaler.fit_transform(train_df), index = train_df.index, columns = train_df.columns)\n",
    "test_df = pd.DataFrame(scaler.transform(test_df), index = test_df.index, columns = test_df.columns)\n",
    "\"\"\"\n",
    "\n",
    "print(f'train_df : \\n{train_df}\\n')\n",
    "print(f'y : \\n{y}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0640a52-ed07-4eb7-91cd-f93aecf4b194",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edcadbe6-9e13-4eb1-a941-e18609e1f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "[LightGBM] [Info] Number of positive: 95, number of negative: 918\n",
      "[LightGBM] [Info] Total Bins 11371\n",
      "[LightGBM] [Info] Number of data points in the train set: 1013, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093781 -> initscore=-2.268320\n",
      "[LightGBM] [Info] Start training from score -2.268320\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's binary_error: 0.000987167\tvalid_1's binary_error: 0.0433071\n",
      "y_pred.sum------------------------94\n",
      "y_train.sum----------------------95.0\n",
      "0.9990128331688055\n",
      "0.9566929133858267\n",
      "Fold : 1\n",
      "[LightGBM] [Info] Number of positive: 83, number of negative: 930\n",
      "[LightGBM] [Info] Total Bins 11429\n",
      "[LightGBM] [Info] Number of data points in the train set: 1013, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.081935 -> initscore=-2.416344\n",
      "[LightGBM] [Info] Start training from score -2.416344\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's binary_error: 0.0621915\tvalid_1's binary_error: 0.102362\n",
      "y_pred.sum------------------------20\n",
      "y_train.sum----------------------83.0\n",
      "0.9378084896347483\n",
      "0.8976377952755905\n",
      "Fold : 2\n",
      "[LightGBM] [Info] Number of positive: 86, number of negative: 928\n",
      "[LightGBM] [Info] Total Bins 11445\n",
      "[LightGBM] [Info] Number of data points in the train set: 1014, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.084813 -> initscore=-2.378684\n",
      "[LightGBM] [Info] Start training from score -2.378684\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0592885\n",
      "y_pred.sum------------------------86\n",
      "y_train.sum----------------------86.0\n",
      "1.0\n",
      "0.9407114624505929\n",
      "Fold : 3\n",
      "[LightGBM] [Info] Number of positive: 91, number of negative: 923\n",
      "[LightGBM] [Info] Total Bins 11417\n",
      "[LightGBM] [Info] Number of data points in the train set: 1014, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.089744 -> initscore=-2.316770\n",
      "[LightGBM] [Info] Start training from score -2.316770\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's binary_error: 0.000986193\tvalid_1's binary_error: 0.055336\n",
      "y_pred.sum------------------------90\n",
      "y_train.sum----------------------91.0\n",
      "0.9990138067061144\n",
      "0.9446640316205533\n",
      "Fold : 4\n",
      "[LightGBM] [Info] Number of positive: 89, number of negative: 925\n",
      "[LightGBM] [Info] Total Bins 11405\n",
      "[LightGBM] [Info] Number of data points in the train set: 1014, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.087771 -> initscore=-2.341157\n",
      "[LightGBM] [Info] Start training from score -2.341157\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's binary_error: 0.00295858\tvalid_1's binary_error: 0.0592885\n",
      "y_pred.sum------------------------86\n",
      "y_train.sum----------------------89.0\n",
      "0.9970414201183432\n",
      "0.9407114624505929\n",
      "----------Result----------\n",
      "Train_acc : [0.9990128331688055, 0.9378084896347483, 1.0, 0.9990138067061144, 0.9970414201183432] , Ave : 0.9865753099256024\n",
      "Valid_acc : [0.9566929133858267, 0.8976377952755905, 0.9407114624505929, 0.9446640316205533, 0.9407114624505929] , Ave : 0.9360835330366312\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#khold\n",
    "cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "models = []\n",
    "\n",
    "# ハイパーパラメータを定義　\n",
    "lgb_params = {\n",
    "    \"objective\":\"binary\",\n",
    "    \"metric\": \"binary_error\",\n",
    "    \"force_row_wise\" : True,\n",
    "    \"seed\" : 0,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_data_in_leaf': 5\n",
    "    }\n",
    "\n",
    "# indexをDateから普通のindexに直す(kholdが使えないため)、日付は消す\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# KFold で学習させる\n",
    "for i ,(trn_index, val_index) in enumerate(cv.split(train_df, y)):\n",
    "    \n",
    "    print(f'Fold : {i}')\n",
    "    X_train ,X_val = train_df.loc[trn_index], train_df.loc[val_index]\n",
    "    y_train ,y_val = y[trn_index],y[val_index]\n",
    "    \n",
    "    # LigthGBM用のデータセットを定義\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_val, y_val)\n",
    "    \n",
    "    model = lgb.train(params = lgb_params, train_set = lgb_train, valid_sets = [lgb_train, lgb_valid], callbacks=[lgb.log_evaluation(period=100), lgb.early_stopping(10)],\n",
    "                     )\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    print(f'y_pred.sum------------------------{np.where(y_pred>=0.5, 1, 0).sum()}')\n",
    "    print(f'y_train.sum----------------------{y_train.sum()}')\n",
    "    \n",
    "    train_acc = accuracy_score(\n",
    "        y_train, np.where(y_pred>=0.5, 1, 0)\n",
    "        )\n",
    "    print(train_acc)\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    y_pred_val = model.predict(X_val)\n",
    "    val_acc = accuracy_score(\n",
    "        y_val, np.where(y_pred_val>=0.5, 1, 0)\n",
    "        )\n",
    "    print(val_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "print('-'*10 + 'Result' +'-'*10)\n",
    "print(f'Train_acc : {train_acc_list} , Ave : {np.mean(train_acc_list)}')\n",
    "print(f'Valid_acc : {val_acc_list} , Ave : {np.mean(val_acc_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e1df5e-497c-48b6-ac94-72a0eebd84d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n",
      "23\n",
      "16\n",
      "11\n",
      "test_pred.sum : 7\n"
     ]
    }
   ],
   "source": [
    "# モデルを用いてテストデータを予測\n",
    "test_pred = np.zeros((len(test_df), 5))\n",
    "for fold_index , gbm in enumerate(models):\n",
    "    pred_test = gbm.predict(test_df)\n",
    "    print(np.where(pred_test>=0.5, 1, 0).sum())\n",
    "    # test_pred[:, fold_index] = pred_test\n",
    "    test_pred[:, fold_index] = np.where(pred_test>=0.5, 1, 0)\n",
    "\n",
    "# 行数で繰り返し予測値を代入\n",
    "test_pred = (np.mean(test_pred, axis=1) > 0.5).astype(int)\n",
    "print(f'test_pred.sum : {test_pred.sum()}')\n",
    "for index, row in sample_sub.iterrows():\n",
    "    sample_sub.iloc[index,1] = test_pred[index]\n",
    "    \n",
    "# 結果を保存\n",
    "sample_sub.to_csv(\"../data/submission_lightBGM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0970c47-d721-44d3-a9a4-70c973924c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
